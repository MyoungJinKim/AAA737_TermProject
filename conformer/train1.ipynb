{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e4bbfc8d",
      "metadata": {},
      "source": [
        "# Stage 1 – CTC pre-training of the Conformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9912bbe",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from datasets import load_dataset, Audio\n",
        "\n",
        "from conformer import Conformer\n",
        "\n",
        "try:\n",
        "    import sentencepiece as spm\n",
        "except ImportError:\n",
        "    spm = None\n",
        "    print(\"sentencepiece is not installed; falling back to the built-in character tokenizer when needed.\")\n",
        "\n",
        "torch.backends.cudnn.benchmark = False\n",
        "logging.basicConfig(level=logging.INFO, format=\"[%(asctime)s] %(message)s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0695e881",
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "config = {\n",
        "    \"experiment_name\": \"stage1_ctc_conformer\",\n",
        "    \"seed\": 1337,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"data\": {\n",
        "        \"sample_rate\": 16_000,\n",
        "        \"text_lowercase\": True,\n",
        "        \"max_audio_seconds\": 20.0,\n",
        "        \"pad_to_max_seconds\": True,\n",
        "        \"train_manifest\": None,\n",
        "        \"valid_manifest\": None,\n",
        "        \"hf_dataset\": {\n",
        "            \"name\": \"parler-tts/mls_eng\",\n",
        "            \"config\": None,\n",
        "            \"train_split\": \"train\",\n",
        "            \"valid_split\": \"validation\",\n",
        "            \"text_column\": \"text\",\n",
        "            \"audio_column\": \"audio\",\n",
        "            \"cache_dir\": \"data/hf-cache\",\n",
        "            \"streaming\": False,\n",
        "        },\n",
        "    },\n",
        "    \"feature_extractor\": {\n",
        "        \"n_mels\": 80,\n",
        "        \"n_fft\": 1024,\n",
        "        \"win_length\": 400,\n",
        "        \"hop_length\": 160,\n",
        "        \"f_min\": 0.0,\n",
        "        \"f_max\": None,\n",
        "        \"mel_power\": 2.0,\n",
        "        \"log_offset\": 1e-6,\n",
        "    },\n",
        "    \"tokenizer\": {\n",
        "        \"use_sentencepiece\": False,\n",
        "        \"sentencepiece_model\": \"tokenizer/stage1_sp.model\",\n",
        "        \"blank_id\": 0,\n",
        "        \"chars\": list(\"abcdefghijklmnopqrstuvwxyz' \"),\n",
        "    },\n",
        "    \"augmentation\": {\n",
        "        \"freq_mask_param\": 15,\n",
        "        \"time_mask_param\": 30,\n",
        "        \"num_freq_masks\": 2,\n",
        "        \"num_time_masks\": 2,\n",
        "        \"prob\": 0.5,\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"input_dim\": 80,\n",
        "        \"encoder_dim\": 512,\n",
        "        \"num_layers\": 3,\n",
        "        \"num_attention_heads\": 8,\n",
        "        \"feed_forward_expansion_factor\": 4,\n",
        "        \"conv_expansion_factor\": 2,\n",
        "        \"conv_kernel_size\": 31,\n",
        "        \"dropout\": 0.1,\n",
        "        \"subsampling_factor\": 8,\n",
        "        \"min_subsample_len_multiplier\": 2,\n",
        "    },\n",
        "    \"dataloader\": {\n",
        "        \"batch_size\": 4,\n",
        "        \"num_workers\": 4,\n",
        "        \"pin_memory\": True,\n",
        "        \"prefetch_factor\": 2,\n",
        "        \"persistent_workers\": False,\n",
        "        \"shuffle\": True,\n",
        "    },\n",
        "    \"optim\": {\n",
        "        \"peak_lr\": 1e-3,\n",
        "        \"weight_decay\": 1e-4,\n",
        "        \"eps\": 1e-9,\n",
        "        \"betas\": (0.9, 0.98),\n",
        "        \"grad_accum_steps\": 4,\n",
        "    },\n",
        "    \"scheduler\": {\n",
        "        \"warmup_steps\": 20_000,\n",
        "        \"total_steps\": 250_000,\n",
        "        \"final_lr_scale\": 0.01,\n",
        "    },\n",
        "    \"trainer\": {\n",
        "        \"num_epochs\": 30,\n",
        "        \"log_interval\": 50,\n",
        "        \"val_interval\": 1,\n",
        "        \"grad_clip\": 5.0,\n",
        "        \"use_amp\": True,\n",
        "        \"checkpoint_dir\": \"checkpoints/stage1\",\n",
        "        \"max_to_keep\": 5,\n",
        "        \"resume_from\": None,\n",
        "    },\n",
        "}\n",
        "\n",
        "set_seed(config[\"seed\"])\n",
        "Path(config[\"trainer\"][\"checkpoint_dir\"]).mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Running Stage 1 on device: {config['device']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfa1c506",
      "metadata": {},
      "source": [
        "## Data & tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "503c7d9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextTokenizer:\n",
        "    def __init__(self, cfg: Dict):\n",
        "        self.blank_id = int(cfg.get(\"blank_id\", 0))\n",
        "        self.lowercase = bool(cfg.get(\"lowercase\", True))\n",
        "        self.use_sentencepiece = bool(cfg.get(\"use_sentencepiece\", False))\n",
        "        sp_model = cfg.get(\"sentencepiece_model\")\n",
        "        if self.use_sentencepiece:\n",
        "            if spm is None:\n",
        "                raise ImportError(\"sentencepiece is required for the requested tokenizer but is not installed.\")\n",
        "            if not sp_model or not Path(sp_model).is_file():\n",
        "                raise FileNotFoundError(f\"SentencePiece model not found: {sp_model}\")\n",
        "            self.processor = spm.SentencePieceProcessor(model_file=str(sp_model))\n",
        "            self.offset = 1 if self.blank_id == 0 else 0\n",
        "            self.vocab_size = self.processor.get_piece_size() + self.offset\n",
        "        else:\n",
        "            base_chars = cfg.get(\"chars\") or list(\"abcdefghijklmnopqrstuvwxyz' \")\n",
        "            symbols: List[str] = []\n",
        "            if self.blank_id == 0:\n",
        "                symbols.append(\"<blank>\")\n",
        "            symbols.extend(base_chars)\n",
        "            self.stoi = {ch: idx for idx, ch in enumerate(symbols)}\n",
        "            self.itos = {idx: ch for ch, idx in self.stoi.items()}\n",
        "            self.vocab_size = len(symbols)\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        if text is None:\n",
        "            return []\n",
        "        text = text.strip()\n",
        "        if self.lowercase:\n",
        "            text = text.lower()\n",
        "        if hasattr(self, \"processor\"):\n",
        "            ids = self.processor.encode(text, out_type=int)\n",
        "            return [idx + self.offset for idx in ids]\n",
        "        tokens: List[int] = []\n",
        "        for ch in text:\n",
        "            idx = self.stoi.get(ch)\n",
        "            if idx is not None and idx != self.blank_id:\n",
        "                tokens.append(idx)\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        if hasattr(self, \"processor\"):\n",
        "            shifted = [idx - self.offset for idx in ids if idx >= self.offset]\n",
        "            return self.processor.decode(shifted)\n",
        "        return \"\".join(self.itos.get(idx, \"\") for idx in ids if idx != self.blank_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71d85735",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LogMelFeatureExtractor(nn.Module):\n",
        "    def __init__(self, sample_rate: int, **kwargs):\n",
        "        super().__init__()\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_mels = kwargs.get(\"n_mels\", 80)\n",
        "        self.melspec = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=sample_rate,\n",
        "            n_fft=kwargs.get(\"n_fft\", 1024),\n",
        "            win_length=kwargs.get(\"win_length\", 400),\n",
        "            hop_length=kwargs.get(\"hop_length\", 160),\n",
        "            f_min=kwargs.get(\"f_min\", 0.0),\n",
        "            f_max=kwargs.get(\"f_max\", None),\n",
        "            power=kwargs.get(\"mel_power\", 2.0),\n",
        "            n_mels=self.n_mels,\n",
        "            norm=None,\n",
        "            mel_scale=\"htk\",\n",
        "        )\n",
        "        self.log_offset = kwargs.get(\"log_offset\", 1e-6)\n",
        "\n",
        "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
        "        if waveform.dim() == 1:\n",
        "            waveform = waveform.unsqueeze(0)\n",
        "        if waveform.size(0) > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "        mel = self.melspec(waveform)\n",
        "        mel = torch.log(torch.clamp(mel, min=self.log_offset))\n",
        "        mel = mel - mel.mean(dim=-1, keepdim=True)\n",
        "        mel = mel / (mel.std(dim=-1, keepdim=True) + 1e-5)\n",
        "        mel = mel.transpose(1, 2).squeeze(0).contiguous()\n",
        "        return mel\n",
        "\n",
        "\n",
        "class SpecAugment:\n",
        "    def __init__(self, freq_mask_param: int, time_mask_param: int, num_freq_masks: int, num_time_masks: int, prob: float = 0.0):\n",
        "        self.freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param) if freq_mask_param > 0 else None\n",
        "        self.time_mask = torchaudio.transforms.TimeMasking(time_mask_param) if time_mask_param > 0 else None\n",
        "        self.num_freq_masks = num_freq_masks\n",
        "        self.num_time_masks = num_time_masks\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, features: torch.Tensor) -> torch.Tensor:\n",
        "        if self.prob <= 0.0 or random.random() > self.prob:\n",
        "            return features\n",
        "        spec = features.transpose(0, 1).unsqueeze(0)\n",
        "        if self.freq_mask is not None:\n",
        "            for _ in range(self.num_freq_masks):\n",
        "                spec = self.freq_mask(spec)\n",
        "        if self.time_mask is not None:\n",
        "            for _ in range(self.num_time_masks):\n",
        "                spec = self.time_mask(spec)\n",
        "        return spec.squeeze(0).transpose(0, 1)\n",
        "\n",
        "\n",
        "class ManifestSpeechDataset(Dataset):\n",
        "    def __init__(self, manifest_path: str, tokenizer: TextTokenizer, feature_extractor: LogMelFeatureExtractor, sample_rate: int, apply_augment: bool = False, augment: Optional[SpecAugment] = None):\n",
        "        self.manifest_path = Path(manifest_path)\n",
        "        if not self.manifest_path.is_file():\n",
        "            raise FileNotFoundError(f\"Manifest not found: {manifest_path}\")\n",
        "        self.tokenizer = tokenizer\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.sample_rate = sample_rate\n",
        "        self.apply_augment = apply_augment\n",
        "        self.augment = augment\n",
        "        self.entries = self._load_manifest()\n",
        "        self._ensure_durations()\n",
        "\n",
        "    def _load_manifest(self) -> List[Dict]:\n",
        "        entries: List[Dict] = []\n",
        "        with self.manifest_path.open() as handle:\n",
        "            for raw_line in handle:\n",
        "                line = raw_line.strip()\n",
        "                if not line or line.startswith(\"#\"):\n",
        "                    continue\n",
        "                parts = line.split(\"\t\")\n",
        "                if len(parts) == 4:\n",
        "                    utt_id, audio_path, duration, transcript = parts\n",
        "                elif len(parts) == 3:\n",
        "                    utt_id, audio_path, transcript = parts\n",
        "                    duration = None\n",
        "                elif len(parts) == 2:\n",
        "                    audio_path, transcript = parts\n",
        "                    utt_id = Path(audio_path).stem\n",
        "                    duration = None\n",
        "                else:\n",
        "                    raise ValueError(f\"Invalid manifest line: {line}\")\n",
        "                path = Path(audio_path)\n",
        "                if not path.is_absolute():\n",
        "                    path = (self.manifest_path.parent / path).resolve()\n",
        "                entries.append(\n",
        "                    {\n",
        "                        \"utt_id\": utt_id,\n",
        "                        \"audio_path\": str(path),\n",
        "                        \"transcript\": transcript.strip(),\n",
        "                        \"duration\": float(duration) if duration else None,\n",
        "                    }\n",
        "                )\n",
        "        if not entries:\n",
        "            raise RuntimeError(f\"Manifest {self.manifest_path} is empty.\")\n",
        "        return entries\n",
        "\n",
        "    def _ensure_durations(self) -> None:\n",
        "        for entry in self.entries:\n",
        "            if entry[\"duration\"] is None:\n",
        "                try:\n",
        "                    info = torchaudio.info(entry[\"audio_path\"])\n",
        "                    entry[\"duration\"] = info.num_frames / info.sample_rate\n",
        "                except Exception:\n",
        "                    entry[\"duration\"] = 0.0\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.entries)\n",
        "\n",
        "    @property\n",
        "    def total_hours(self) -> float:\n",
        "        total_seconds = sum(e.get(\"duration\", 0.0) or 0.0 for e in self.entries)\n",
        "        return total_seconds / 3600.0\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict:\n",
        "        entry = self.entries[idx]\n",
        "        waveform, sr = torchaudio.load(entry[\"audio_path\"])\n",
        "        waveform = waveform.to(torch.float32)\n",
        "        if sr != self.sample_rate:\n",
        "            waveform = torchaudio.functional.resample(waveform, sr, self.sample_rate)\n",
        "        features = self.feature_extractor(waveform)\n",
        "        if self.apply_augment and self.augment is not None:\n",
        "            features = self.augment(features)\n",
        "        tokens = torch.tensor(self.tokenizer.encode(entry[\"transcript\"]), dtype=torch.long)\n",
        "        duration = entry.get(\"duration\")\n",
        "        if duration is None:\n",
        "            duration = waveform.size(-1) / self.sample_rate\n",
        "        return {\n",
        "            \"features\": features,\n",
        "            \"feature_length\": features.size(0),\n",
        "            \"tokens\": tokens,\n",
        "            \"token_length\": int(tokens.size(0)),\n",
        "            \"utt_id\": entry[\"utt_id\"],\n",
        "            \"seconds\": duration,\n",
        "        }\n",
        "\n",
        "\n",
        "class HuggingFaceSpeechDataset(Dataset):\n",
        "    def __init__(self, dataset_cfg: Dict, split: str, tokenizer: TextTokenizer, feature_extractor: LogMelFeatureExtractor, sample_rate: int, apply_augment: bool = False, augment: Optional[SpecAugment] = None, target_seconds: Optional[float] = None, pad_to_target: bool = False):\n",
        "        if dataset_cfg.get(\"streaming\", False):\n",
        "            raise ValueError(\"Streaming datasets are not supported in this notebook; please disable streaming.\")\n",
        "        self.dataset_name = dataset_cfg[\"name\"]\n",
        "        self.dataset_cfg = dataset_cfg\n",
        "        self.split = split\n",
        "        self.tokenizer = tokenizer\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.sample_rate = sample_rate\n",
        "        self.apply_augment = apply_augment\n",
        "        self.augment = augment\n",
        "        self.audio_column = dataset_cfg.get(\"audio_column\", \"audio\")\n",
        "        self.text_column = dataset_cfg.get(\"text_column\", \"text\")\n",
        "        self.target_seconds = target_seconds\n",
        "        self.pad_to_target = pad_to_target\n",
        "        self.target_num_frames = int(round(target_seconds * sample_rate)) if target_seconds else None\n",
        "        load_kwargs = {\n",
        "            \"split\": split,\n",
        "            \"cache_dir\": dataset_cfg.get(\"cache_dir\"),\n",
        "        }\n",
        "        config_name = dataset_cfg.get(\"config\")\n",
        "        if config_name:\n",
        "            ds = load_dataset(self.dataset_name, config_name, **load_kwargs)\n",
        "        else:\n",
        "            ds = load_dataset(self.dataset_name, **load_kwargs)\n",
        "        ds = ds.cast_column(self.audio_column, Audio(sampling_rate=self.sample_rate))\n",
        "        if self.text_column not in ds.column_names:\n",
        "            raise ValueError(f\"Column '{self.text_column}' not found in dataset columns {ds.column_names}.\")\n",
        "        if self.audio_column not in ds.column_names:\n",
        "            raise ValueError(f\"Column '{self.audio_column}' not found in dataset columns {ds.column_names}.\")\n",
        "        self.dataset = ds\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.dataset.num_rows\n",
        "\n",
        "    @property\n",
        "    def total_hours(self) -> float:\n",
        "        if self.target_num_frames is not None:\n",
        "            fixed_seconds = self.target_num_frames / self.sample_rate\n",
        "            return len(self) * fixed_seconds / 3600.0\n",
        "        return 0.0\n",
        "\n",
        "    def _fix_duration(self, waveform: torch.Tensor) -> torch.Tensor:\n",
        "        if self.target_num_frames is None:\n",
        "            return waveform\n",
        "        num_frames = waveform.size(-1)\n",
        "        if num_frames > self.target_num_frames:\n",
        "            waveform = waveform[..., : self.target_num_frames]\n",
        "        elif num_frames < self.target_num_frames and self.pad_to_target:\n",
        "            pad = self.target_num_frames - num_frames\n",
        "            waveform = F.pad(waveform, (0, pad))\n",
        "        return waveform\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict:\n",
        "        example = self.dataset[idx]\n",
        "        audio_dict = example[self.audio_column]\n",
        "        waveform = torch.tensor(audio_dict[\"array\"], dtype=torch.float32)\n",
        "        if waveform.dim() == 1:\n",
        "            waveform = waveform.unsqueeze(0)\n",
        "        waveform = self._fix_duration(waveform)\n",
        "        features = self.feature_extractor(waveform)\n",
        "        if self.apply_augment and self.augment is not None:\n",
        "            features = self.augment(features)\n",
        "        tokens = torch.tensor(self.tokenizer.encode(example[self.text_column]), dtype=torch.long)\n",
        "        duration = self.target_seconds if self.target_seconds else waveform.size(-1) / self.sample_rate\n",
        "        return {\n",
        "            \"features\": features,\n",
        "            \"feature_length\": features.size(0),\n",
        "            \"tokens\": tokens,\n",
        "            \"token_length\": int(tokens.size(0)),\n",
        "            \"utt_id\": str(example.get(\"id\", idx)),\n",
        "            \"seconds\": duration,\n",
        "        }\n",
        "\n",
        "\n",
        "class SpeechDataCollator:\n",
        "    def __init__(self, pad_to_multiple_of: Optional[int] = None, subsampling_factor: int = 1, min_subsample_len_multiplier: int = 1):\n",
        "        self.pad_to_multiple_of = pad_to_multiple_of\n",
        "        self.subsampling_factor = max(1, subsampling_factor)\n",
        "        self.min_subsample_frames = max(1, self.subsampling_factor * max(1, min_subsample_len_multiplier))\n",
        "\n",
        "    def _is_usable(self, sample: Dict) -> bool:\n",
        "        if sample[\"token_length\"] == 0:\n",
        "            return False\n",
        "        if sample[\"feature_length\"] < self.min_subsample_frames:\n",
        "            return False\n",
        "        approx_logits = max(1, (sample[\"feature_length\"] // self.subsampling_factor) - 1)\n",
        "        if approx_logits < sample[\"token_length\"]:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def __call__(self, batch: List[Dict]):\n",
        "        filtered = [sample for sample in batch if self._is_usable(sample)]\n",
        "        if not filtered:\n",
        "            return None\n",
        "        feat_dim = filtered[0][\"features\"].size(1)\n",
        "        max_len = max(sample[\"feature_length\"] for sample in filtered)\n",
        "        if self.pad_to_multiple_of and max_len % self.pad_to_multiple_of != 0:\n",
        "            max_len = ((max_len // self.pad_to_multiple_of) + 1) * self.pad_to_multiple_of\n",
        "        features = torch.zeros(len(filtered), max_len, feat_dim, dtype=torch.float32)\n",
        "        input_lengths = torch.zeros(len(filtered), dtype=torch.long)\n",
        "        target_lengths = torch.zeros(len(filtered), dtype=torch.long)\n",
        "        targets: List[torch.Tensor] = []\n",
        "        utt_ids: List[str] = []\n",
        "        for idx, sample in enumerate(filtered):\n",
        "            length = sample[\"feature_length\"]\n",
        "            features[idx, :length] = sample[\"features\"]\n",
        "            input_lengths[idx] = length\n",
        "            target_lengths[idx] = sample[\"token_length\"]\n",
        "            targets.append(sample[\"tokens\"])\n",
        "            utt_ids.append(sample[\"utt_id\"])\n",
        "        targets = torch.cat(targets)\n",
        "        return features, input_lengths, targets, target_lengths, utt_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb906c01",
      "metadata": {},
      "outputs": [],
      "source": [
        "class WarmupExponentialDecayScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, warmup_steps: int, total_steps: int, final_lr_scale: float, last_epoch: int = -1):\n",
        "        self.warmup_steps = max(1, warmup_steps)\n",
        "        self.total_steps = max(total_steps, warmup_steps + 1)\n",
        "        self.final_lr_scale = final_lr_scale\n",
        "        super().__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        step = max(1, self.last_epoch + 1)\n",
        "        if step <= self.warmup_steps:\n",
        "            scale = step / self.warmup_steps\n",
        "        else:\n",
        "            progress = min(1.0, (step - self.warmup_steps) / (self.total_steps - self.warmup_steps))\n",
        "            scale = math.exp(math.log(self.final_lr_scale) * progress)\n",
        "        return [base_lr * scale for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.total = 0.0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, value: float, n: int = 1):\n",
        "        self.total += value * n\n",
        "        self.count += n\n",
        "\n",
        "    @property\n",
        "    def avg(self) -> float:\n",
        "        return self.total / max(1, self.count)\n",
        "\n",
        "\n",
        "def save_checkpoint(state: Dict, checkpoint_dir: str, max_to_keep: int) -> Path:\n",
        "    checkpoint_dir = Path(checkpoint_dir)\n",
        "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "    ckpt_path = checkpoint_dir / f\"epoch{state['epoch']:02d}_val{state['val_loss']:.4f}.pt\"\n",
        "    torch.save(state, ckpt_path)\n",
        "    checkpoints = sorted(checkpoint_dir.glob(\"epoch*.pt\"))\n",
        "    if len(checkpoints) > max_to_keep:\n",
        "        for stale in checkpoints[:-max_to_keep]:\n",
        "            stale.unlink(missing_ok=True)\n",
        "    return ckpt_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b39f383",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model: nn.Module, dataloader: DataLoader, criterion, optimizer, scheduler, scaler: GradScaler, device: torch.device, epoch: int, global_step: int, grad_accum_steps: int, grad_clip: float, log_interval: int, amp_enabled: bool) -> Tuple[int, float]:\n",
        "    model.train()\n",
        "    loss_meter = AverageMeter()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    steps_in_accum = 0\n",
        "    start_time = time.time()\n",
        "    skipped_batches = 0\n",
        "\n",
        "    def _optimizer_step():\n",
        "        nonlocal global_step, steps_in_accum\n",
        "        if steps_in_accum == 0:\n",
        "            return\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        scheduler.step()\n",
        "        global_step += 1\n",
        "        steps_in_accum = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader, start=1):\n",
        "        if batch is None:\n",
        "            continue\n",
        "        features, input_lengths, targets, target_lengths, _ = batch\n",
        "        features = features.to(device)\n",
        "        input_lengths = input_lengths.to(device)\n",
        "        targets = targets.to(device)\n",
        "        target_lengths = target_lengths.to(device)\n",
        "        with autocast(enabled=amp_enabled):\n",
        "            logits, logit_lengths = model(features, input_lengths)\n",
        "        if torch.any(target_lengths > logit_lengths):\n",
        "            skipped_batches += 1\n",
        "            continue\n",
        "        with autocast(enabled=amp_enabled):\n",
        "            loss = criterion(logits.transpose(0, 1), targets, logit_lengths, target_lengths)\n",
        "            loss = loss / grad_accum_steps\n",
        "        scaler.scale(loss).backward()\n",
        "        steps_in_accum += 1\n",
        "        loss_meter.update(loss.item() * grad_accum_steps, n=features.size(0))\n",
        "        if steps_in_accum == grad_accum_steps:\n",
        "            _optimizer_step()\n",
        "            if global_step > 0 and global_step % log_interval == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "                logging.info(\n",
        "                    f\"Epoch {epoch:02d} | step {global_step} | loss {loss_meter.avg:.4f} | lr {current_lr:.2e} | {elapsed:.1f}s\"\n",
        "                )\n",
        "                start_time = time.time()\n",
        "\n",
        "    if steps_in_accum > 0:\n",
        "        _optimizer_step()\n",
        "    if skipped_batches:\n",
        "        logging.info(f\"Epoch {epoch:02d} skipped {skipped_batches} batches due to insufficient subsampled frames.\")\n",
        "    return global_step, loss_meter.avg\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module, dataloader: DataLoader, criterion, device: torch.device, amp_enabled: bool) -> float:\n",
        "    model.eval()\n",
        "    loss_meter = AverageMeter()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "            features, input_lengths, targets, target_lengths, _ = batch\n",
        "            features = features.to(device)\n",
        "            input_lengths = input_lengths.to(device)\n",
        "            targets = targets.to(device)\n",
        "            target_lengths = target_lengths.to(device)\n",
        "            with autocast(enabled=amp_enabled):\n",
        "                logits, logit_lengths = model(features, input_lengths)\n",
        "                if torch.any(target_lengths > logit_lengths):\n",
        "                    continue\n",
        "                loss = criterion(logits.transpose(0, 1), targets, logit_lengths, target_lengths)\n",
        "            loss_meter.update(loss.item(), n=features.size(0))\n",
        "    return loss_meter.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9da32680",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_dataloader(dataset: Dataset, collate_fn, loader_cfg: Dict, shuffle: bool) -> DataLoader:\n",
        "    kwargs = {\n",
        "        \"batch_size\": loader_cfg[\"batch_size\"],\n",
        "        \"num_workers\": loader_cfg[\"num_workers\"],\n",
        "        \"pin_memory\": loader_cfg[\"pin_memory\"],\n",
        "        \"persistent_workers\": loader_cfg[\"persistent_workers\"] and loader_cfg[\"num_workers\"] > 0,\n",
        "        \"collate_fn\": collate_fn,\n",
        "        \"drop_last\": False,\n",
        "    }\n",
        "    if loader_cfg[\"num_workers\"] > 0 and loader_cfg.get(\"prefetch_factor\"):\n",
        "        kwargs[\"prefetch_factor\"] = loader_cfg[\"prefetch_factor\"]\n",
        "    return DataLoader(dataset, shuffle=shuffle, **kwargs)\n",
        "\n",
        "\n",
        "def build_dataset(cfg: Dict, tokenizer: TextTokenizer, feature_extractor: LogMelFeatureExtractor, split: str, apply_augment: bool, augment: Optional[SpecAugment]):\n",
        "    data_cfg = cfg[\"data\"]\n",
        "    hf_cfg = data_cfg.get(\"hf_dataset\")\n",
        "    if hf_cfg:\n",
        "        target_seconds = data_cfg.get(\"max_audio_seconds\")\n",
        "        pad_to_target = data_cfg.get(\"pad_to_max_seconds\", False)\n",
        "        return HuggingFaceSpeechDataset(\n",
        "            hf_cfg,\n",
        "            split=split,\n",
        "            tokenizer=tokenizer,\n",
        "            feature_extractor=feature_extractor,\n",
        "            sample_rate=data_cfg[\"sample_rate\"],\n",
        "            apply_augment=apply_augment,\n",
        "            augment=augment,\n",
        "            target_seconds=target_seconds,\n",
        "            pad_to_target=pad_to_target,\n",
        "        )\n",
        "    manifest_key = \"train_manifest\" if split == \"train\" else \"valid_manifest\"\n",
        "    manifest_path = data_cfg.get(manifest_key)\n",
        "    if not manifest_path:\n",
        "        raise ValueError(\"No dataset source configured. Provide Hugging Face settings or manifest paths.\")\n",
        "    return ManifestSpeechDataset(\n",
        "        manifest_path,\n",
        "        tokenizer,\n",
        "        feature_extractor,\n",
        "        data_cfg[\"sample_rate\"],\n",
        "        apply_augment=apply_augment,\n",
        "        augment=augment,\n",
        "    )\n",
        "\n",
        "\n",
        "def format_hours(hours: float) -> str:\n",
        "    if hours and hours > 0:\n",
        "        return f\"~{hours:.2f}h\"\n",
        "    return \"n/a\"\n",
        "\n",
        "\n",
        "def run_training(cfg: Dict) -> Dict:\n",
        "    device = torch.device(cfg[\"device\"])\n",
        "    tokenizer_cfg = dict(cfg[\"tokenizer\"])\n",
        "    tokenizer_cfg.setdefault(\"lowercase\", cfg[\"data\"].get(\"text_lowercase\", True))\n",
        "    tokenizer = TextTokenizer(tokenizer_cfg)\n",
        "    feature_kwargs = dict(cfg[\"feature_extractor\"])\n",
        "    train_extractor = LogMelFeatureExtractor(sample_rate=cfg[\"data\"][\"sample_rate\"], **feature_kwargs)\n",
        "    valid_extractor = LogMelFeatureExtractor(sample_rate=cfg[\"data\"][\"sample_rate\"], **feature_kwargs)\n",
        "    augment = SpecAugment(**cfg[\"augmentation\"]) if cfg[\"augmentation\"].get(\"prob\", 0.0) > 0 else None\n",
        "    train_dataset = build_dataset(cfg, tokenizer, train_extractor, split=cfg[\"data\"].get(\"hf_dataset\", {}).get(\"train_split\", \"train\"), apply_augment=True, augment=augment)\n",
        "    valid_split = cfg[\"data\"].get(\"hf_dataset\", {}).get(\"valid_split\", \"validation\")\n",
        "    valid_dataset = build_dataset(cfg, tokenizer, valid_extractor, split=valid_split, apply_augment=False, augment=None)\n",
        "    subsampling_factor = max(1, cfg[\"model\"].get(\"subsampling_factor\", 1))\n",
        "    min_subsample_len_multiplier = cfg[\"model\"].get(\"min_subsample_len_multiplier\", 1)\n",
        "    collate_fn = SpeechDataCollator(\n",
        "        pad_to_multiple_of=subsampling_factor,\n",
        "        subsampling_factor=subsampling_factor,\n",
        "        min_subsample_len_multiplier=min_subsample_len_multiplier,\n",
        "    )\n",
        "    train_loader = build_dataloader(train_dataset, collate_fn, cfg[\"dataloader\"], shuffle=cfg[\"dataloader\"].get(\"shuffle\", True))\n",
        "    valid_loader = build_dataloader(valid_dataset, collate_fn, cfg[\"dataloader\"], shuffle=False)\n",
        "\n",
        "    hours_train = format_hours(getattr(train_dataset, \"total_hours\", 0.0))\n",
        "    hours_valid = format_hours(getattr(valid_dataset, \"total_hours\", 0.0))\n",
        "    frame_ms = cfg[\"feature_extractor\"].get(\"hop_length\", 160) / cfg[\"data\"][\"sample_rate\"] * 1000\n",
        "    effective_stride = frame_ms * subsampling_factor\n",
        "    logging.info(\n",
        "        f\"Train set: {len(train_dataset)} utterances ({hours_train}), \"\n",
        "        f\"Valid set: {len(valid_dataset)} utterances ({hours_valid})\"\n",
        "    )\n",
        "    logging.info(\n",
        "        f\"Subsampling factor {subsampling_factor} ⇒ encoder frame rate ≈ {effective_stride:.1f} ms\"\n",
        "    )\n",
        "\n",
        "    num_classes = tokenizer.vocab_size\n",
        "    model = Conformer(\n",
        "        num_classes=num_classes,\n",
        "        input_dim=cfg[\"model\"][\"input_dim\"],\n",
        "        encoder_dim=cfg[\"model\"][\"encoder_dim\"],\n",
        "        num_encoder_layers=cfg[\"model\"][\"num_layers\"],\n",
        "        num_attention_heads=cfg[\"model\"][\"num_attention_heads\"],\n",
        "        feed_forward_expansion_factor=cfg[\"model\"][\"feed_forward_expansion_factor\"],\n",
        "        conv_expansion_factor=cfg[\"model\"][\"conv_expansion_factor\"],\n",
        "        conv_kernel_size=cfg[\"model\"][\"conv_kernel_size\"],\n",
        "        input_dropout_p=cfg[\"model\"][\"dropout\"],\n",
        "        feed_forward_dropout_p=cfg[\"model\"][\"dropout\"],\n",
        "        attention_dropout_p=cfg[\"model\"][\"dropout\"],\n",
        "        conv_dropout_p=cfg[\"model\"][\"dropout\"],\n",
        "    ).to(device)\n",
        "    logging.info(f\"Conformer parameters: {model.count_parameters():,}\")\n",
        "\n",
        "    criterion = nn.CTCLoss(blank=tokenizer.blank_id, zero_infinity=True)\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=cfg[\"optim\"][\"peak_lr\"],\n",
        "        betas=cfg[\"optim\"][\"betas\"],\n",
        "        eps=cfg[\"optim\"][\"eps\"],\n",
        "        weight_decay=cfg[\"optim\"][\"weight_decay\"],\n",
        "    )\n",
        "    scheduler = WarmupExponentialDecayScheduler(\n",
        "        optimizer,\n",
        "        warmup_steps=cfg[\"scheduler\"][\"warmup_steps\"],\n",
        "        total_steps=cfg[\"scheduler\"][\"total_steps\"],\n",
        "        final_lr_scale=cfg[\"scheduler\"][\"final_lr_scale\"],\n",
        "    )\n",
        "    amp_enabled = bool(cfg[\"trainer\"][\"use_amp\"] and torch.cuda.is_available())\n",
        "    scaler = GradScaler(enabled=amp_enabled)\n",
        "\n",
        "    start_epoch = 1\n",
        "    global_step = 0\n",
        "    best_val = float(\"inf\")\n",
        "    best_path: Optional[Path] = None\n",
        "    resume_path = cfg[\"trainer\"].get(\"resume_from\")\n",
        "    if resume_path:\n",
        "        ckpt = torch.load(resume_path, map_location=device)\n",
        "        model.load_state_dict(ckpt[\"model_state\"])\n",
        "        optimizer.load_state_dict(ckpt[\"optim_state\"])\n",
        "        scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
        "        if \"scaler_state\" in ckpt and amp_enabled and ckpt[\"scaler_state\"] is not None:\n",
        "            scaler.load_state_dict(ckpt[\"scaler_state\"])\n",
        "        start_epoch = ckpt[\"epoch\"] + 1\n",
        "        global_step = ckpt.get(\"global_step\", 0)\n",
        "        best_val = ckpt.get(\"best_val\", best_val)\n",
        "        best_path = Path(resume_path)\n",
        "        logging.info(f\"Resumed from {resume_path} (epoch {ckpt['epoch']})\")\n",
        "\n",
        "    for epoch in range(start_epoch, cfg[\"trainer\"][\"num_epochs\"] + 1):\n",
        "        global_step, train_loss = train_one_epoch(\n",
        "            model,\n",
        "            train_loader,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "            scheduler,\n",
        "            scaler,\n",
        "            device,\n",
        "            epoch,\n",
        "            global_step,\n",
        "            cfg[\"optim\"][\"grad_accum_steps\"],\n",
        "            cfg[\"trainer\"][\"grad_clip\"],\n",
        "            cfg[\"trainer\"][\"log_interval\"],\n",
        "            amp_enabled,\n",
        "        )\n",
        "        if epoch % cfg[\"trainer\"][\"val_interval\"] == 0:\n",
        "            val_loss = evaluate(model, valid_loader, criterion, device, amp_enabled)\n",
        "            improved = val_loss < best_val\n",
        "            if improved:\n",
        "                best_val = val_loss\n",
        "            ckpt_state = {\n",
        "                \"epoch\": epoch,\n",
        "                \"global_step\": global_step,\n",
        "                \"val_loss\": float(val_loss),\n",
        "                \"best_val\": float(best_val),\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"optim_state\": optimizer.state_dict(),\n",
        "                \"scheduler_state\": scheduler.state_dict(),\n",
        "                \"scaler_state\": scaler.state_dict() if amp_enabled else None,\n",
        "                \"config\": cfg,\n",
        "            }\n",
        "            ckpt_path = save_checkpoint(ckpt_state, cfg[\"trainer\"][\"checkpoint_dir\"], cfg[\"trainer\"][\"max_to_keep\"])\n",
        "            if improved:\n",
        "                best_path = ckpt_path\n",
        "            logging.info(\n",
        "                f\"Epoch {epoch:02d} | train loss {train_loss:.4f} | val loss {val_loss:.4f} | best {best_val:.4f}\"\n",
        "            )\n",
        "        else:\n",
        "            logging.info(f\"Epoch {epoch:02d} | train loss {train_loss:.4f}\")\n",
        "    return {\"best_val_loss\": best_val, \"best_checkpoint\": str(best_path) if best_path else None, \"global_step\": global_step}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88e0e554",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    trainer_state = run_training(config)\n",
        "    print(trainer_state)\n",
        "except Exception as exc:\n",
        "    print(f\"Training loop aborted: {exc}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
