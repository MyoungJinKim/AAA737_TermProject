from __future__ import annotations
import logging
import json
import contextlib
import random
from typing import Dict, Any

from .utils import StoppingCriteriaSub
import torch
import torch.nn as nn
from transformers import LlamaTokenizer, StoppingCriteriaList, LlamaForCausalLM, AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, TaskType, get_peft_model


# ê¸°ì¡´: from conformer import Conformer  (âŒ í˜„ì¬ ê²½ë¡œ êµ¬ì¡°ì™€ ì•ˆ ë§ìŒ)
# ìˆ˜ì •: ê°™ì€ íŒ¨í‚¤ì§€(model) ì•ˆì˜ ì„œë¸ŒíŒ¨í‚¤ì§€ì—ì„œ ìƒëŒ€ import
from .conformer.conformer.model import Conformer, ConformerEncoderOnly, load_encoder_from_checkpoint

from .SpeechLlamaProj import SpeechLlamaProj

class modelYIM(nn.Module):
    # -----------------------------
    # Conformer ì´ˆê¸°í™”
    # -----------------------------
    def _init_conformer(
        self,
        input_dim: int,
        encoder_dim: int,
        num_encoder_layers: int,
        num_attention_heads: int,
        feed_forward_expansion_factor: int,
        conv_expansion_factor: int,
        conv_kernel_size: int,
        dropout: float,
        modelpath: str,
    ) -> ConformerEncoderOnly:
        """
        config["model"]ì˜ ì„¤ì •ì— ë§ì¶° Conformer encoderë¥¼ ì´ˆê¸°í™”.
        í•„ìš”í•˜ë‹¤ë©´ modelpath(=checkpoint)ë¥¼ ë‚´ë¶€ì—ì„œ ë¡œë“œí•˜ëŠ” Conformer êµ¬í˜„ì´ë¼ê³  ê°€ì •.
        """
        conformer = ConformerEncoderOnly(
            input_dim=input_dim,
            encoder_dim=encoder_dim,
            num_encoder_layers=num_encoder_layers,
            num_attention_heads=num_attention_heads,
            feed_forward_expansion_factor=feed_forward_expansion_factor,
            conv_expansion_factor=conv_expansion_factor,
            conv_kernel_size=conv_kernel_size,
            input_dropout_p=dropout,
            feed_forward_dropout_p=dropout,
            attention_dropout_p=dropout,
            conv_dropout_p=dropout,
        )
        load_encoder_from_checkpoint(
            checkpoint_path=modelpath, model=conformer)
        return conformer

    
    # -----------------------------
    # autocast í—¬í¼
    # -----------------------------
    def maybe_autocast(self, dtype=torch.float16):
        """
        CPUë©´ autocast ë¹„í™œì„±í™”, GPUë©´ ì§€ì • dtypeìœ¼ë¡œ autocast í™œì„±í™”.
        """
        enable_autocast = self.device.type == "cuda"
        if enable_autocast:
            return torch.cuda.amp.autocast(dtype=dtype)
        else:
            return contextlib.nullcontext()

    def __init__(
        self,
        # ---- LLaMA / Conformer ê²½ë¡œ ----
        llama_path: str = "",
        conformer_path: str = "",

        # ---- Conformer êµ¬ì¡° ----
        conformer_dim: int = 512,
        conformer_layers: int = 12,
        conformer_input_dim: int = 80,
        conformer_concat_num: int = 3,
        num_attention_heads: int = 8,
        feed_forward_expansion_factor: int = 4,
        conv_expansion_factor: int = 2,
        conv_kernel_size: int = 31,
        dropout: float = 0.1,
        subsampling_factor: int = 8,
        min_subsample_len_multiplier: int = 2,

        # ---- Speech â†’ LLaMA projection ----
        speech_llama_proj_model: str = "",
        freeze_speech_llama_proj: bool = False,

        # ---- LoRA / LLaMA í•™ìŠµ ì„¤ì • ----
        lora: bool = True,
        lora_rank: int = 8,
        lora_alpha: int = 32,
        lora_dropout: float = 0.1,
        train_llama: bool = False,

        max_txt_len: int = 128,
    ):
        
        # config ê°’ ë³´ê´€ (í•„ìš” ì‹œ forward ë“±ì—ì„œ ì‚¬ìš©)
        self.conformer_dim = conformer_dim
        self.conformer_layers = conformer_layers
        self.conformer_input_dim = conformer_input_dim
        self.conformer_concat_num = conformer_concat_num
        self.num_attention_heads = num_attention_heads
        self.feed_forward_expansion_factor = feed_forward_expansion_factor
        self.conv_expansion_factor = conv_expansion_factor
        self.conv_kernel_size = conv_kernel_size
        self.dropout = dropout
        self.subsampling_factor = subsampling_factor
        self.min_subsample_len_multiplier = min_subsample_len_multiplier

        self.lora = lora
        self.train_llama = train_llama
        self.max_txt_len = max_txt_len

        super().__init__()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.lora = lora
        self.train_llama = train_llama
        self.max_txt_len = max_txt_len

        if not llama_path:
            raise ValueError("The 'llama_path' in config.yaml is empty. Please specify the path to your LLaMA model (e.g., 'meta-llama/Llama-2-7b-hf' or a local path).")

        logging.info(f'Loading LLaMA Tokenizer from {llama_path}')
        self.llama_tokenizer = AutoTokenizer.from_pretrained(llama_path, use_fast=False)
        self.llama_tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        self.llama_tokenizer.padding_side = "right"

        logging.info('Loading LLaMA Model')
        self.llama_model = AutoModelForCausalLM.from_pretrained(
            llama_path,
            torch_dtype=torch.float16,
        )
        self.llama_model.resize_token_embeddings(len(self.llama_tokenizer))
        logging.info('Loading LLaMA Done')

        # ğŸ”¹ í•™ìŠµ ëª¨ë“œ ê²°ì • ë¡œì§
        if self.lora:
            # LoRAë¥¼ ì“°ëŠ” ê²½ìš°: base LLaMAëŠ” freeze, LoRA ëª¨ë“ˆë§Œ í•™ìŠµ
            for name, param in self.llama_model.named_parameters():
                param.requires_grad = False
            logging.info('Base LLaMA is frozen. LoRA adapters will be trainable.')

            self.peft_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                inference_mode=False,
                r=lora_rank,
                lora_alpha=lora_alpha,
                lora_dropout=lora_dropout,
            )
            self.llama_model = get_peft_model(self.llama_model, self.peft_config)
            self.llama_model.print_trainable_parameters()
            logging.info('LoRA Training Enabled')

        else:
            # LoRAë¥¼ ì“°ì§€ ì•ŠëŠ” ê²½ìš°: train_llama flagë¡œ ì „ì²´ LLaMA í•™ìŠµ ì—¬ë¶€ ê²°ì •
            if not self.train_llama:
                for name, param in self.llama_model.named_parameters():
                    param.requires_grad = False
                logging.info('LLaMA is frozen (no LoRA, no full fine-tuning).')
            else:
                logging.info('Full LLaMA fine-tuning is enabled (no LoRA).')


        assert conformer_path
        logging.info('Loading Conformer Model')
        self.conformer = self._init_conformer(
            modelpath=conformer_path,
            input_dim=conformer_input_dim,
            encoder_dim=conformer_dim,
            num_encoder_layers=conformer_layers,
            num_attention_heads=num_attention_heads,
            feed_forward_expansion_factor=feed_forward_expansion_factor,
            conv_expansion_factor=conv_expansion_factor,
            conv_kernel_size=conv_kernel_size,
            dropout=dropout,
        )
        # Conformer ì¶œë ¥ì— ëŒ€í•´ layer norm ì ìš© (ì°¨ì›: conformer_dim)
        self.ln_speech = nn.LayerNorm(conformer_dim)

    
        logging.info('Loading speech LLAMA proj')
        logging.info("Initializing speech LLAMA proj")
        in_dim = conformer_dim * conformer_concat_num
        out_dim = self.llama_model.config.hidden_size

        self.speech_llama_proj = SpeechLlamaProj(
            in_dim=in_dim,
            out_dim=out_dim,
            pretrained_path=speech_llama_proj_model if speech_llama_proj_model else None,
            freeze=freeze_speech_llama_proj,
            key_in_ckpt="speech_llama_proj",         # ckpt êµ¬ì¡°ì— ë§ê²Œ ì¡°ì • ê°€ëŠ¥
        )


    def forward(self, samples, verbose: bool = False):

        # ------------------------------------------------------------------
        # 1) ì˜¤ë””ì˜¤ ì¸ì½”ë”© (Conformer)
        #   - collatorê°€ ë§Œë“¤ì–´ ì¤€ key ì‚¬ìš©:
        #       "input_features":      [B, T_max, F]
        #       "input_input_lengths": [B]
        # ------------------------------------------------------------------
        """
            "input_features": feats,  # [T_i, F]
            "feature_length": feat_len,
            "text": text_value,
            "utt_id": utt_id,
        }
        """
        features = samples["input_features"]          # [B, T_max, F]
        input_lengths = samples["feature_length"]  # [B]
        texts = samples["text"]                      # List[str]

        # ë””ë°”ì´ìŠ¤ ì •ë ¬ (prepare_sampleì—ì„œ ì´ë¯¸ ì˜®ê²¼ë‹¤ë©´ ì¤‘ë³µë  ìˆ˜ ìˆìŒ)
        features = features.to(self.device)
        input_lengths = input_lengths.to(self.device)

        # Conformer: (B, T_max, F) + length[B] â†’ (B, T_enc, C_enc), out_lengths[B]
        # stage1ì—ì„œ ì“°ë˜ ì‹œê·¸ë‹ˆì²˜: outputs, output_lengths = model(inputs, input_lengths)
        speech_embeds, out_lengths = self.conformer(features, input_lengths)
        # speech_embeds: [B, T_enc, C_enc]

        # ì—¬ê¸°ì„œ ìœ„ì˜ speech embedsë¥¼ time ë§ˆë‹¤ conformer_concat_numë§Œí¼ concatí•œ ë’¤ LLaMA ì°¨ì›ìœ¼ë¡œ ë°”ê¿”ì¤˜ì•¼ í•¨

        B, T_enc, C_enc = speech_embeds.shape
        conformer_concat_num = getattr(self, "conformer_concat_num", 1)

        if conformer_concat_num > 1:
            # T_encì´ concat_numì˜ ë°°ìˆ˜ê°€ ì•„ë‹ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë’¤ë¥¼ ì˜ë¼ì„œ ë§ì¶¤
            T_trim = (T_enc // conformer_concat_num) * conformer_concat_num
            if T_trim != T_enc:
                speech_embeds = speech_embeds[:, :T_trim, :]
                out_lengths = out_lengths.clamp(max=T_trim)

                T_enc = T_trim

            T_new = T_enc // conformer_concat_num  # concat í›„ í† í° ê°œìˆ˜
            # (B, T_enc, C_enc) â†’ (B, T_new, C_enc * concat_num)
            concat_embeds = speech_embeds.view(
                B,
                T_new,
                C_enc * conformer_concat_num,
            )
            speech_token_lengths = (out_lengths // conformer_concat_num)
        else:
            # concat_num = 1 ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            concat_embeds = speech_embeds
            T_new = T_enc
            speech_token_lengths = out_lengths


        # ëª¨ë“  í”„ë ˆì„ì„ ìœ íš¨ í† í°ìœ¼ë¡œ ì‚¬ìš© â†’ attention mask = 1
        # (B, T_new)
        speech_embeds = self.speech_llama_proj(concat_embeds)
        max_T = speech_embeds.size(1)
        idx = torch.arange(max_T, device=self.device).unsqueeze(0)  # [1, T_new]
        speech_atts = (idx < speech_token_lengths.unsqueeze(1)).long()  # [B, T_new]

        # prepare inputs for LLM
        text_with_eos = [t + self.llama_tokenizer.eos_token for t in texts]

        to_regress_tokens = self.llama_tokenizer(
            text_with_eos,
            return_tensors="pt",
            padding="longest",
            truncation=True,
            max_length=self.max_txt_len,
            add_special_tokens=False,
        ).to(self.device)

        to_regress_embeds = self.llama_model.model.embed_tokens(to_regress_tokens.input_ids) if not self.lora else self.llama_model.model.model.embed_tokens(to_regress_tokens.input_ids)
        targets = to_regress_tokens.input_ids.masked_fill(
            to_regress_tokens.input_ids == self.llama_tokenizer.pad_token_id, -100
        )

        # ------------------------------------------------------------------
        # 5) BOS + speech + text â†’ LLaMA ì…ë ¥ êµ¬ì„±
        # ------------------------------------------------------------------
        B = speech_embeds.size(0)

        # BOS í† í° id: (B, 1)
        bos_ids = torch.full(
            (B, 1),
            fill_value=self.llama_tokenizer.bos_token_id,
            dtype=to_regress_tokens.input_ids.dtype,
            device=self.device,
        )

        if not self.lora:
            bos_embeds = self.llama_model.model.embed_tokens(bos_ids)
        else:
            bos_embeds = self.llama_model.model.model.embed_tokens(bos_ids)

        # BOS ìë¦¬ì— ëŒ€ì‘í•˜ëŠ” attention mask: (B, 1)
        atts_bos = torch.ones(
            (B, 1),
            dtype=speech_atts.dtype,
            device=self.device,
        )

        # LLaMAìš© input_embeds: [BOS] + [SPEECH] + [TEXT]
        # shape: (B, 1 + T_new + T_txt, hidden_llama)
        inputs_embeds = torch.cat(
            [bos_embeds, speech_embeds, to_regress_embeds],
            dim=1,
        )

        # attention mask: (B, 1 + T_new + T_txt)
        attention_mask = torch.cat(
            [atts_bos, speech_atts, to_regress_tokens.attention_mask],
            dim=1,
        )

        # íƒ€ê¹ƒ: BOS + speech êµ¬ê°„ì€ lossë¥¼ ê³„ì‚°í•˜ì§€ ì•Šë„ë¡ -100
        # empty_targets: (B, 1 + T_new)
        empty_targets = torch.full(
            (B, 1 + T_new),
            fill_value=-100,
            dtype=torch.long,
            device=self.device,
        )
        # ìµœì¢… targets: [bos+speech] = -100, ê·¸ ë’¤ í…ìŠ¤íŠ¸ íƒ€ê¹ƒ
        # shape: (B, 1 + T_new + T_txt)
        targets_full = torch.cat([empty_targets, targets], dim=1)

        # ------------------------------------------------------------------
        # 6) LLaMA forward + loss ê³„ì‚°
        # ------------------------------------------------------------------
        with self.maybe_autocast():
            outputs = self.llama_model(
                inputs_embeds=inputs_embeds,
                attention_mask=attention_mask,
                return_dict=True,
                labels=targets_full,
            )
            loss = outputs.loss

        if verbose:
            # ë””ì½”ë”© ì •í™•ë„ ê³„ì‚° (í…ìŠ¤íŠ¸ êµ¬ê°„ë§Œ)
            nvocab = self.llama_model.config.vocab_size

            # logitsì—ì„œ [bos+speech] êµ¬ê°„ì„ ê±´ë„ˆë›°ê³  í…ìŠ¤íŠ¸ êµ¬ê°„ë§Œ ê°€ì ¸ì˜¤ê¸°
            offset = empty_targets.size(1)  # bos+speech length
            # í…ìŠ¤íŠ¸ ìœ„ì¹˜ì˜ logits: [B, T_txt, V]
            text_logits = outputs.logits[:, offset:-1, :]  # shift í•œ ì¹¸ ê³ ë ¤ (ë§ˆì§€ë§‰ í† í° ì˜ˆì¸¡ì€ ì‹¤ì œ ë¼ë²¨ ì—†ìŒ)
            pred_ids = text_logits.contiguous().view(-1, nvocab).argmax(dim=-1)

            labels = targets_full[:, offset:].contiguous().view(-1)
            mask = (labels != -100)
            correct = (pred_ids[mask] == labels[mask]).float().sum()
            total = mask.sum().item()

            return {"loss": loss, "correct": correct, "total": total}

        return outputs
    
    def generate(self, samples, generate_cfg):
        """
        samples:
        - "input_features":      [B, T_max, F]
        - "input_input_lengths": [B]

        generate_cfg: dict
        - max_new_tokens, num_beams, do_sample, min_length, temperature,
            top_p, repetition_penalty, length_penalty ë“±

        """

        # ------------------------------------------------------------------
        # 1) ì˜¤ë””ì˜¤ ì¸ì½”ë”© (Conformer) - forwardì™€ ë™ì¼í•œ ì…ë ¥ í˜•ì‹ ì‚¬ìš©
        # ------------------------------------------------------------------
        features = samples["input_features"].to(self.device)          # [B, T_max, F]
        input_lengths = samples["input_input_lengths"].to(self.device)  # [B]
        batch_size = features.size(0)

        # Conformer: (B, T_max, F) + length[B] â†’ (B, T_enc, C_enc), out_lengths[B]
        speech_embeds, out_lengths = self.conformer(features, input_lengths)
        # speech_embeds: [B, T_enc, C_enc]

        B, T_enc, C_enc = speech_embeds.shape
        conformer_concat_num = getattr(self, "conformer_concat_num", 1)

        # ------------------------------------------------------------------
        # 2) time-axis concat (stacking) + ê¸¸ì´ ë°˜ì˜ (forwardì™€ ë™ì¼ ë¡œì§)
        # ------------------------------------------------------------------
        if conformer_concat_num > 1:
            # T_encì´ concat_numì˜ ë°°ìˆ˜ê°€ ì•„ë‹ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë’¤ë¥¼ ì˜ë¼ì„œ ë§ì¶¤
            T_trim = (T_enc // conformer_concat_num) * conformer_concat_num
            if T_trim != T_enc:
                speech_embeds = speech_embeds[:, :T_trim, :]
                out_lengths = out_lengths.clamp(max=T_trim)
                T_enc = T_trim

            T_new = T_enc // conformer_concat_num  # concat í›„ í† í° ê°œìˆ˜

            # (B, T_enc, C_enc) â†’ (B, T_new, C_enc * concat_num)
            concat_embeds = speech_embeds.view(
                B,
                T_new,
                C_enc * conformer_concat_num,
            )

            # ê¸¸ì´ë„ concat ì´í›„ í† í° ë‹¨ìœ„ë¡œ ë³€í™˜
            speech_token_lengths = (out_lengths // conformer_concat_num)  # [B]
        else:
            concat_embeds = speech_embeds
            T_new = T_enc
            speech_token_lengths = out_lengths  # [B]

        # ------------------------------------------------------------------
        # 3) Speech â†’ LLaMA projection + speech attention mask
        # ------------------------------------------------------------------
        # (ì›í•˜ë©´ ln_speech ë¨¼ì € ì ìš© ê°€ëŠ¥)
        # concat_embeds = self.ln_speech(concat_embeds)

        # (B, T_new, C_enc * concat_num) â†’ (B, T_new, hidden_llama)
        speech_embeds = self.speech_llama_proj(concat_embeds)  # [B, T_new, H_llama]

        # speech attention mask: padding ì œì™¸, 1=ìœ íš¨, 0=pad
        max_T = speech_embeds.size(1)
        idx = torch.arange(max_T, device=self.device).unsqueeze(0)       # [1, T_new]
        speech_atts = (idx < speech_token_lengths.unsqueeze(1)).long()   # [B, T_new]

        # ------------------------------------------------------------------
        # 4) BOS + (ì„ íƒ) í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ + speech â†’ LLaMA generate ì…ë ¥ êµ¬ì„±
        # ------------------------------------------------------------------
        # BOS í† í° id í…ì„œ: (B, 1)
        bos_ids = torch.full(
            (batch_size, 1),
            fill_value=self.llama_tokenizer.bos_token_id,
            dtype=torch.long,
            device=self.device,
        )

        if not self.lora:
            bos_embeds = self.llama_model.model.embed_tokens(bos_ids)  # [B, 1, H]
        else:
            bos_embeds = self.llama_model.model.model.embed_tokens(bos_ids)

        # BOS attention mask: (B,1)
        atts_bos = torch.ones(
            (batch_size, 1),
            dtype=torch.long,
            device=self.device,
        )

        # ê¸°ë³¸: [BOS] + [SPEECH]
        embeds = torch.cat([bos_embeds, speech_embeds], dim=1)  # [B, 1+T_new, H]
        attns = torch.cat([atts_bos, speech_atts], dim=1)       # [B, 1+T_new]

        eos_id = self.llama_tokenizer.eos_token_id
        stop_words_ids = [torch.tensor([eos_id], device=self.device)]
        stopping_criteria = StoppingCriteriaList(
            [StoppingCriteriaSub(stops=stop_words_ids)]
        )

        outputs = self.llama_model.generate(
            inputs_embeds=embeds,
            attention_mask=attns,
            max_new_tokens=generate_cfg.get("max_new_tokens", 200),
            stopping_criteria=stopping_criteria,
            num_beams=generate_cfg.get("num_beams", 4),
            do_sample=generate_cfg.get("do_sample", False),
            min_length=generate_cfg.get("min_length", 1),
            temperature=generate_cfg.get("temperature", 1.0),
            top_p=generate_cfg.get("top_p", 0.9),
            repetition_penalty=generate_cfg.get("repetition_penalty", 1.0),
            length_penalty=generate_cfg.get("length_penalty", 1.0),
        )

        # special tokens(EOS ë“±)ì€ ë¹¼ê³  ë””ì½”ë“œí•˜ëŠ” ê²Œ ì¼ë°˜ì 
        text = self.llama_tokenizer.batch_decode(
            outputs, skip_special_tokens=True
        )

        return text


    # ----------------------------------------------------------------------
    # config(dict)ì—ì„œ ë°”ë¡œ ì´ˆê¸°í™”í•  ìˆ˜ ìˆëŠ” helper
    # ----------------------------------------------------------------------
    @classmethod
    def from_config(cls, config: Dict[str, Any]) -> "modelYIM":
        """
        config: ë³´í†µ config["model"] ë¸”ë¡ì„ ê·¸ëŒ€ë¡œ ë„˜ê²¨ ë°›ëŠ”ë‹¤ê³  ê°€ì •.
        ì˜ˆ:
            model_cfg = full_cfg["model"]
            model = modelYIM.from_config(model_cfg)
        """
        llama_path = config.get("llama_path", "")
        conformer_path = config.get("conformer_path", "")

        conformer_dim = config.get("conformer_dim", 512)
        conformer_layers = config.get("conformer_layers", 12)
        conformer_input_dim = config.get("conformer_input_dim", 80)
        conformer_concat_num = config.get("conformer_concat_num", 3)

        feed_forward_expansion_factor = config.get("feed_forward_expansion_factor", 4)
        conv_expansion_factor = config.get("conv_expansion_factor", 2)
        conv_kernel_size = config.get("conv_kernel_size", 31)
        dropout = config.get("dropout", 0.1)
        subsampling_factor = config.get("subsampling_factor", 8)
        min_subsample_len_multiplier = config.get("min_subsample_len_multiplier", 2)
        num_attention_heads = config.get("num_attention_heads", 8)

        speech_llama_proj_model = config.get("speech_llama_proj_model", "")
        freeze_speech_llama_proj = config.get("freeze_speech_llama_proj", False)

        lora = config.get("lora", True)
        lora_rank = config.get("lora_rank", 8)
        lora_alpha = config.get("lora_alpha", 32)
        lora_dropout = config.get("lora_dropout", 0.1)
        train_llama = config.get("train_llama", False)
        max_txt_len = config.get("max_txt_len", 128)

        model = cls(
            llama_path=llama_path,
            conformer_path=conformer_path,
            conformer_dim=conformer_dim,
            conformer_layers=conformer_layers,
            conformer_input_dim=conformer_input_dim,
            conformer_concat_num=conformer_concat_num,
            num_attention_heads=num_attention_heads,
            feed_forward_expansion_factor=feed_forward_expansion_factor,
            conv_expansion_factor=conv_expansion_factor,
            conv_kernel_size=conv_kernel_size,
            dropout=dropout,
            subsampling_factor=subsampling_factor,
            min_subsample_len_multiplier=min_subsample_len_multiplier,
            speech_llama_proj_model=speech_llama_proj_model,
            freeze_speech_llama_proj=freeze_speech_llama_proj,
            lora=lora,
            lora_rank=lora_rank,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            train_llama=train_llama,
            max_txt_len=max_txt_len,
        )


        ckpt_path = config.get("ckpt", "")
        if ckpt_path:
            logging.info("Load modelYIM ckpt from: {}".format(ckpt_path))
            ckpt = torch.load(ckpt_path, map_location="cpu")
            # stage1 Conformer / LLaMA weight êµ¬ì¡°ì™€ ì¶©ëŒí•˜ì§€ ì•Šê²Œ strict=False
            model.load_state_dict(ckpt.get("model", ckpt), strict=False)

        return model
            

# class modelYIM(nn.Module):
#     @classmethod
#     def init_conformer(self, input_dim=80, encoder_dim=512, num_encoder_layers=12, modelpath = " "):

#         conformer = Conformer(input_dim=input_dim, 
#                   encoder_dim=encoder_dim, 
#                   num_encoder_layers=num_encoder_layers,
#                   modelpath=modelpath).to(self.device)
#         return conformer
    
#     def maybe_autocast(self, dtype=torch.float16):
#         # if on cpu, don't use autocast
#         # if on gpu, use autocast with dtype if provided, otherwise use torch.float16
#         enable_autocast = self.device != torch.device("cpu")

#         if enable_autocast:
#             return torch.cuda.amp.autocast(dtype=dtype)
#         else:
#             return contextlib.nullcontext()

#     def __init__(
#         self,
#         llama_path="",
#         conformer_path="",
#         conformer_dim=512,
#         conformer_layers=12,
#         conformer_input_dim=80,
#         conformer_concat_num = 3,
#         speech_llama_proj_model="",
#         freeze_speech_llama_proj=False,

#         lora=True,
#         lora_rank=8,
#         lora_alpha=32,
#         lora_dropout=0.1,
#         train_llama: bool = False,

#         max_txt_len=128,
#     ):
#         super().__init__()
#         self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#         self.lora = lora
#         self.train_llama = train_llama
#         self.max_txt_len = max_txt_len

#         logging.info('Loading LLaMA Tokenizer')
#         self.llama_tokenizer = LlamaTokenizer.from_pretrained(llama_path, use_fast=False)
#         self.llama_tokenizer.add_special_tokens({'pad_token': '[PAD]'})
#         self.llama_tokenizer.padding_side = "right"

#         logging.info('Loading LLaMA Model')
#         self.llama_model = LlamaForCausalLM.from_pretrained(
#             llama_path,
#             torch_dtype=torch.float16,
#         )
#         self.llama_model.resize_token_embeddings(len(self.llama_tokenizer))
#         logging.info('Loading LLaMA Done')

#         # ğŸ”¹ í•™ìŠµ ëª¨ë“œ ê²°ì • ë¡œì§
#         if self.lora:
#             # LoRAë¥¼ ì“°ëŠ” ê²½ìš°: base LLaMAëŠ” freeze, LoRA ëª¨ë“ˆë§Œ í•™ìŠµ
#             for name, param in self.llama_model.named_parameters():
#                 param.requires_grad = False
#             logging.info('Base LLaMA is frozen. LoRA adapters will be trainable.')

#             self.peft_config = LoraConfig(
#                 task_type=TaskType.CAUSAL_LM,
#                 inference_mode=False,
#                 r=lora_rank,
#                 lora_alpha=lora_alpha,
#                 lora_dropout=lora_dropout,
#             )
#             self.llama_model = get_peft_model(self.llama_model, self.peft_config)
#             self.llama_model.print_trainable_parameters()
#             logging.info('LoRA Training Enabled')

#         else:
#             # LoRAë¥¼ ì“°ì§€ ì•ŠëŠ” ê²½ìš°: train_llama flagë¡œ ì „ì²´ LLaMA í•™ìŠµ ì—¬ë¶€ ê²°ì •
#             if not self.train_llama:
#                 for name, param in self.llama_model.named_parameters():
#                     param.requires_grad = False
#                 logging.info('LLaMA is frozen (no LoRA, no full fine-tuning).')
#             else:
#                 logging.info('Full LLaMA fine-tuning is enabled (no LoRA).')


#         assert conformer_path
#         logging.info('Loading Conformer Model')
#         self.conformer = self.init_conformer(modelpath=conformer_path, input_dim=conformer_input_dim, encoder_dim=conformer_dim, num_encoder_layers=conformer_layers)
#         self.ln_speech = nn.LayerNorm(self.conformer.config.d_model)

    
#         logging.info('Loading speech LLAMA proj')
#         logging.info("Initializing speech LLAMA proj")
#         in_dim = conformer_dim * conformer_concat_num
#         out_dim = self.llama_model.config.hidden_size

#         self.speech_llama_proj = SpeechLlamaProj(
#             in_dim=in_dim,
#             out_dim=out_dim,
#             pretrained_path=speech_llama_proj_model,  # ì—†ìœ¼ë©´ None
#             freeze=freeze_speech_llama_proj,
#             key_in_ckpt="speech_llama_proj",         # ckpt êµ¬ì¡°ì— ë§ê²Œ ì¡°ì • ê°€ëŠ¥
#         )


#     def forward(self, samples, verbose=False):

#         # use speech/audio encoder to encode speech/audio
#         spectrogram = samples["spectrogram"]
#         raw_wav = samples.get("raw_wav", None)
#         audio_padding_mask = samples.get("padding_mask", None)


#         #(B, T', encoder_dim)
#         # intput shape ë§Œë“¤ì–´ì•¼ í•¨ 
#         speech_embeds = self.conformer(spectrogram, raw_wav=raw_wav, audio_padding_mask=audio_padding_mask)

#         # ì—¬ê¸°ì„œ ìœ„ì˜ speech embedsë¥¼ time ë§ˆë‹¤ conformer_concat_numë§Œí¼ concatí•œ ë’¤ LLaMA ì°¨ì›ìœ¼ë¡œ ë°”ê¿”ì¤˜ì•¼ í•¨

#         B, T_enc, C_enc = speech_embeds.shape
#         conformer_concat_num = getattr(self, "conformer_concat_num", 1)

#         if conformer_concat_num > 1:
#             # T_encì´ concat_numì˜ ë°°ìˆ˜ê°€ ì•„ë‹ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë’¤ë¥¼ ì˜ë¼ì„œ ë§ì¶¤
#             T_trim = (T_enc // conformer_concat_num) * conformer_concat_num
#             if T_trim != T_enc:
#                 speech_embeds = speech_embeds[:, :T_trim, :]
#                 T_enc = T_trim

#             T_new = T_enc // conformer_concat_num  # concat í›„ í† í° ê°œìˆ˜
#             # (B, T_enc, C_enc) â†’ (B, T_new, C_enc * concat_num)
#             concat_embeds = speech_embeds.view(
#                 B,
#                 T_new,
#                 C_enc * conformer_concat_num,
#             )
#         else:
#             # concat_num = 1 ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
#             concat_embeds = speech_embeds
#             T_new = T_enc

#         # ëª¨ë“  í”„ë ˆì„ì„ ìœ íš¨ í† í°ìœ¼ë¡œ ì‚¬ìš© â†’ attention mask = 1
#         # (B, T_new)
#         speech_embeds = self.speech_llama_proj(concat_embeds)
#         speech_atts = torch.ones(
#             speech_embeds.size()[:-1],
#             dtype=torch.long,
#             device=self.device,
#         )

#         # prepare inputs for LLM
#         text = [t + self.llama_tokenizer.eos_token for t in samples["text"]]

#         to_regress_tokens = self.llama_tokenizer(
#             text,
#             return_tensors="pt",
#             padding="longest",
#             truncation=True,
#             max_length=self.max_txt_len,
#             add_special_tokens=False
#         ).to(self.device)

#         to_regress_embeds = self.llama_model.model.embed_tokens(to_regress_tokens.input_ids) if not self.lora else self.llama_model.model.model.embed_tokens(to_regress_tokens.input_ids)
#         targets = to_regress_tokens.input_ids.masked_fill(
#             to_regress_tokens.input_ids == self.llama_tokenizer.pad_token_id, -100
#         )
#         # ìŒì„± êµ¬ê°„(bos + speech_embeds)ì— ëŒ€í•´ì„œëŠ” lossë¥¼ ê³„ì‚°í•˜ì§€ ì•Šê¸° ìœ„í•´
#         # ê¸¸ì´: (B, 1 + T_new), ê°’ì€ ì „ë¶€ -100
#         empty_targets = (
#             torch.ones(
#                 [speech_atts.shape[0], speech_atts.shape[1] + 1],
#                 dtype=torch.long,
#                 device=self.device,
#             ).fill_(-100)
#         )
#         # ìµœì¢… targets: [bos + speech] êµ¬ê°„ì€ -100, ê·¸ ë’¤ í…ìŠ¤íŠ¸ í† í°ì€ ì‹¤ì œ label
#         # shape: (B, 1 + T_new + T_txt)
#         targets = torch.cat([empty_targets, targets], dim=1)


#         # 4) BOS í† í° + ìŒì„± ì„ë² ë”© + í…ìŠ¤íŠ¸ ì„ë² ë”© â†’ LLaMA ì…ë ¥ êµ¬ì„±
#         batch_size = speech_embeds.shape[0]

#         # BOS í† í° id í…ì„œ: (B, 1)
#         bos = torch.ones(
#             [batch_size, 1],
#             dtype=to_regress_tokens.input_ids.dtype,
#             device=self.device,
#         ) * self.llama_tokenizer.bos_token_id

#         bos_embeds = self.llama_model.model.embed_tokens(bos) if not self.lora else self.llama_model.model.model.embed_tokens(bos)
#         atts_bos = speech_atts[:, :1]

#         # ìµœì¢… ì…ë ¥ ì„ë² ë”©: [BOS] + [speech] + [text]
#         # shape: (B, 1 + T_new + T_txt, hidden_llama)
#         inputs_embeds = torch.cat(
#             [bos_embeds, speech_embeds, to_regress_embeds],
#             dim=1,
#         )

#         # ìµœì¢… attention mask: (B, 1 + T_new + T_txt)
#         attention_mask = torch.cat(
#             [atts_bos, speech_atts, to_regress_tokens.attention_mask],
#             dim=1,
#         )

#         # calulate loss
#         with self.maybe_autocast():
#             outputs = self.llama_model(
#                 inputs_embeds=inputs_embeds,
#                 attention_mask=attention_mask,
#                 return_dict=True,
#                 labels=targets,
#             )
#             loss = outputs.loss

#         if verbose:
#             nvocab = self.llama_model.config.vocab_size
#             results = outputs.logits[:, empty_targets.size(1) - 1: -1, :].contiguous().view(-1, nvocab).argmax(dim=-1)
#             labels = targets[:, empty_targets.size(1):].contiguous().view(-1)
#             mask = (labels != -100)
#             correct = (results[mask] == labels[mask]).float().sum()
#             total = len(labels[mask])

#         if verbose:
#             return {"loss": loss, "correct": correct, "total": total}

#         return outputs
    
#     def generate(self, samples, generate_cfg, prompts=None):
#         batch_size = samples["spectrogram"].shape[0]

#         spectrogram = samples["spectrogram"]
#         raw_wav = samples.get("raw_wav", None)
#         audio_padding_mask = samples.get("padding_mask", None)


#         #(B, T', encoder_dim)
#         # intput shape ë§Œë“¤ì–´ì•¼ í•¨ 
#         speech_embeds = self.conformer(spectrogram, raw_wav=raw_wav, audio_padding_mask=audio_padding_mask)

#         # ì—¬ê¸°ì„œ ìœ„ì˜ speech embedsë¥¼ time ë§ˆë‹¤ conformer_concat_numë§Œí¼ concatí•œ ë’¤ LLaMA ì°¨ì›ìœ¼ë¡œ ë°”ê¿”ì¤˜ì•¼ í•¨

#         B, T_enc, C_enc = speech_embeds.shape
#         conformer_concat_num = getattr(self, "conformer_concat_num", 1)

#         if conformer_concat_num > 1:
#             # T_encì´ concat_numì˜ ë°°ìˆ˜ê°€ ì•„ë‹ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë’¤ë¥¼ ì˜ë¼ì„œ ë§ì¶¤
#             T_trim = (T_enc // conformer_concat_num) * conformer_concat_num
#             if T_trim != T_enc:
#                 speech_embeds = speech_embeds[:, :T_trim, :]
#                 T_enc = T_trim

#             T_new = T_enc // conformer_concat_num  # concat í›„ í† í° ê°œìˆ˜
#             # (B, T_enc, C_enc) â†’ (B, T_new, C_enc * concat_num)
#             concat_embeds = speech_embeds.view(
#                 B,
#                 T_new,
#                 C_enc * conformer_concat_num,
#             )
#         else:
#             # concat_num = 1 ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
#             concat_embeds = speech_embeds
#             T_new = T_enc

#         # ëª¨ë“  í”„ë ˆì„ì„ ìœ íš¨ í† í°ìœ¼ë¡œ ì‚¬ìš© â†’ attention mask = 1
#         # (B, T_new)
#         speech_embeds = self.speech_llama_proj(concat_embeds)
#         speech_atts = torch.ones(
#             speech_embeds.size()[:-1],
#             dtype=torch.long,
#             device=self.device,
#         )
#         bos = torch.ones(
#             [batch_size, 1],
#             dtype=torch.int32,
#             device=speech_embeds.device,
#         ) * self.llama_tokenizer.bos_token_id
#         bos_embeds = self.llama_model.model.embed_tokens(bos) if not self.lora else self.llama_model.model.model.embed_tokens(bos)
#         atts_bos = speech_atts[:, :1]

#         embeds = torch.cat([bos_embeds, speech_embeds], dim=1)
#         attns = torch.cat([atts_bos, speech_atts], dim=1)

#         stop_words_ids = [torch.tensor([2]).cuda()]  
#         stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])
#         outputs = self.llama_model.generate(
#             inputs_embeds=embeds,
#             max_new_tokens=generate_cfg.get("max_new_tokens", 200),
#             stopping_criteria=stopping_criteria,
#             num_beams=generate_cfg.get("num_beams", 4),
#             do_sample=generate_cfg.get("do_sample", False),
#             min_length=generate_cfg.get("min_length", 1),
#             temperature=generate_cfg.get("temperature", 1.0),
#             top_p=generate_cfg.get("top_p", 0.9),
#             repetition_penalty=generate_cfg.get("repetition_penalty", 1.0),
#             length_penalty=generate_cfg.get("length_penalty", 1.0),
#             attention_mask=attns,
#         )
#         text = self.llama_tokenizer.batch_decode(outputs, add_special_tokens=False)

#         return text

#     @classmethod
#     def from_config(cls, config):

#         llama_path = config.get("llama_path")
#         conformer_path = config.get("conformer_path", "")

#         conformer_dim = config.get("conformer_dim", 512)
#         conformer_layers = config.get("conformer_layers", 12)
#         conformer_input_dim = config.get("conformer_input_dim", 80)
#         conformer_concat_num = config.get("conformer_concat_num", 3)

#         speech_llama_proj_model = config.get("speech_llama_proj_model", "")
#         freeze_speech_llama_proj = config.get("freeze_speech_llama_proj", False)

#         lora = config.get("lora", True)
#         lora_rank = config.get("lora_rank", 8)
#         lora_alpha = config.get("lora_alpha", 32)
#         lora_dropout = config.get("lora_dropout", 0.1)
#         train_llama = config.get("train_llama", False)   # ğŸ”¹ ì¶”ê°€
#         max_txt_len = config.get("max_txt_len", 128)

#         model = cls(
#             llama_path=llama_path,
#             conformer_path=conformer_path,
#             conformer_dim=conformer_dim,
#             conformer_layers=conformer_layers,
#             conformer_input_dim=conformer_input_dim,
#             conformer_concat_num=conformer_concat_num,
#             speech_llama_proj_model=speech_llama_proj_model,
#             freeze_speech_llama_proj=freeze_speech_llama_proj,
#             lora=lora,
#             lora_rank=lora_rank,
#             lora_alpha=lora_alpha,
#             lora_dropout=lora_dropout,
#             train_llama=train_llama,
#             max_txt_len=max_txt_len,
#         )

#         ckpt_path = config.get("ckpt", "")
#         if ckpt_path:
#             logging.info("Load modelYIM ckpt from: {}".format(ckpt_path))
#             ckpt = torch.load(ckpt_path, map_location="cpu")
#             model.load_state_dict(ckpt['model'], strict=False)

#         return model
            
