# Copyright (2024) Tsinghua University, Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# /data_x/aa007878/deep/myung/configs/config.yaml

experiment_name: "stage2"
seed: 1337
device: "cuda"   # train.py에서 필요하면 torch.cuda.is_available()로 재조정

data:
  sample_rate: 16000
  # text_lowercase: true
  max_audio_seconds: 20.0
  pad_to_max_seconds: true
  hf_dataset:
    name: "parler-tts/mls_eng"
    config: null
    train_split: "train"
    valid_split: "dev"
    text_column: "transcript"
    audio_column: "audio"
    cache_dir: "data/hf-cache"
    streaming: false

feature_extractor:
  n_mels: 80
  n_fft: 1024
  win_length: 400
  hop_length: 160
  f_min: 0.0
  f_max: null
  mel_power: 2.0
  log_offset: 1.0e-6



dataloader:
  batch_size: 32
  num_workers: 32
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: false
  shuffle: true
  # 필요하면 encoder stride에 맞춰 패딩 배수를 맞출 수 있음 (예: 8)
  pad_to_multiple_of: 8

# optim:
#   peak_lr: 1.0e-3
#   weight_decay: 1.0e-4
#   eps: 1.0e-9
#   betas: [0.9, 0.98]
#   grad_accum_steps: 4

# scheduler:
#   warmup_steps: 20000
#   total_steps: 250000
#   final_lr_scale: 0.01

# trainer:
#   num_epochs: 30
#   log_interval: 50
#   val_interval: 1
#   grad_clip: 5.0
#   use_amp: true
#   max_to_keep: 5
#   resume_from: null   # 이어서 학습할 체크포인트 경로 (없으면 null)

#########################
# LLaMA / Conformer 연결
#########################
model:
  # LLaMA 모델 경로 (HF 모델 이름 또는 로컬 디렉터리)
  llama_path: "meta-llama/Llama-3.2-1B-Instruct"  # 예: "meta-llama/Meta-Llama-3-8B-Instruct" 또는 "/data_x/.../llama"

  # stage1 / 사전학습된 Conformer checkpoint 경로
  conformer_path: "/data_x/aa007878/deep/myung/model/conformer/conformer_model/conformer_stage1_layer12.pt"  # 예: "/data_x/aa007878/deep/myung/checkpoints/stage1/epochxx_....pt"

  # 오디오 인코더(Conformer) 구조 관련 파라미터
  conformer_dim: 512              # Conformer encoder 출력 차원 (LLM projection 입력 차원)
  conformer_layers: 12            # 사용할 encoder 레이어 수 (stage2에서 상단 몇 개만 쓴다면 여기에 맞춰 조정)
  conformer_input_dim: 80         # Mel feature dim (n_mels와 동일)
  conformer_concat_num: 3         # 시간 축 stacking 개수 (예: 3 프레임 concat → 3 * 80 차원 등)
  feed_forward_expansion_factor: 4
  conv_expansion_factor: 2
  conv_kernel_size: 31
  dropout: 0.1
  subsampling_factor: 8
  min_subsample_len_multiplier: 2
  num_attention_heads: 8

  # Audio-LLM projection (SpeechLlamaProj) 관련
  speech_llama_proj_model: ""     # projection만 따로 저장된 checkpoint 경로 (없으면 빈 문자열)
  freeze_speech_llama_proj: false # true면 projection 가중치 freeze

  # LoRA / LLaMA 학습 관련
  lora: true          # LoRA 사용 여부
  lora_rank: 8
  lora_alpha: 32
  lora_dropout: 0.1

  train_llama: false  # true로 두면 LLaMA 본체도 gradient 업데이트
  max_txt_len: 128    # text 토큰 길이 최대값 (LLM 입력 쪽에서 잘라 쓰는 길이)
#########################


run:
  # log & settings
  # seed: 42
  output_dir: "output directory"
  model_storage_path: "/data_x/aa007878/deep/myung/model/model_storage"
  evaluate: False # if True, only evaluate model on test data

  wandb:
    enable: true
    project: "conformer-stage2"
    entity: null
    run_name: "layer_8_batch_32_{modelpath}"

  log_freq: 50
  epoch_based: True
  # iters_per_epoch: 3000
  accum_grad_iters: 1
  val_step_interval: 1000  # 1000 스텝마다 validation 수행 (0이면 epoch 단위로만 수행)
  # batch_size_train: 32
  # batch_size_eval: 32
  # num_workers: 32

  device: "cuda"
  use_distributed: True
  amp: True
  world_size: 1
  dist_url: "env://"

  # optimizer & scheduler
  optims:
    max_epoch: 30
    warmup_steps: 8000
    warmup_start_lr: 1e-6
    init_lr: 1e-4
    min_lr: 1e-5
    weight_decay: 0.05
    beta2: 0.999